# -*- coding: utf-8 -*-
"""Model_Architecture.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gROsCCwWB3JRXqkEwK4xAe3q3kM1BsfM
"""

import os
import collections
import string
import pandas as pd
from matplotlib import pyplot as plt
from matplotlib.pyplot import figure, imshow, axis
from matplotlib.image import imread
import re
import time

import tqdm
from tqdm import tqdm
from tqdm import trange

import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.utils  import *

import numpy as np
from collections import Counter   # Dict subclass for counting hashable items

import pickle
from pickle import dump


import cv2

# https://www.tensorflow.org/tutorials/text/image_captioning

class Resnet_Encoder(tf.keras.Model):

    # This encoder passes the features through a Fully connected layer
    def __init__(self, embedding_dim):
        super(Resnet_Encoder, self).__init__()

        # shape after fc : (batch_size, 64, embedding_dim)
        self.fc = tf.keras.layers.Dense(embedding_dim)
        self.dropout = tf.keras.layers.Dropout(0.5, noise_shape=None, seed=None)

    def call(self, x):
        x= self.dropout(x)
        x = self.fc(x)
        x = tf.nn.relu(x)
        return x

#____________________________________________________________________________________________

class SeqEmbedding(tf.keras.layers.Layer):
  def __init__(self, vocab_size, max_length,embedding_dim,  depth):
    super().__init__()
    self.pos_embedding = tf.keras.layers.Embedding(input_dim=max_length, output_dim=embedding_dim)

    self.token_embedding = tf.keras.layers.Embedding(
        input_dim=vocab_size,
        input_length= max_length,
        output_dim=embedding_dim,
        )
    self.embedding_dim = embedding_dim
    self.add = tf.keras.layers.Add()

#_______________________________________________________________________________________________________
  def call(self, seq):
    seq = self.token_embedding(seq) # (batch, seq, embedding_dim)
    batch, seq_len = tf.shape(seq)[0], tf.shape(seq)[1]

    x = tf.range(seq_len)  # (seq)

    x = self.pos_embedding(x)  # (1, seq, depth)
    x = tf.broadcast_to(x, [batch, seq_len, self.embedding_dim])


    return self.add([seq, x])

#________________________________________________________________________________________________________

class CausalSelfAttention(tf.keras.layers.Layer):
  def __init__(self, **kwargs):
    super().__init__()
    self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)
    # Use Add instead of + so the keras mask propagates through.
    self.add = tf.keras.layers.Add()
    self.layernorm = tf.keras.layers.LayerNormalization()

#______________________________________________________________________________________________________
  def call(self, seqs):
    attention_output = self.mha(query=seqs, value=seqs,       # will use 'value' for both 'key' and 'value'
                                                  use_causal_mask=True)
    seqs = self.add([seqs, attention_output])
    return self.layernorm(seqs)

#_____________________________________________________________________________________________________

class CrossAttention(tf.keras.layers.Layer):
  def __init__(self,**kwargs):
    super().__init__()
    self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)
    self.add = tf.keras.layers.Add()
    self.layernorm = tf.keras.layers.LayerNormalization()

#______________________________________________________________________________________________________
  def call(self, seqs, features, **kwargs):
    attention_output, attention_scores = self.mha(
                                                  query=seqs, value=features,
                                                  return_attention_scores=True)

    self.last_attention_scores = attention_scores

    seqs = self.add([seqs, attention_output])
    return self.layernorm(seqs)

#___________________________________________________________________________________________________________

class FeedForward(tf.keras.layers.Layer):
  def __init__(self, units, dropout_rate=0.1):
    super().__init__()
    self.seq = tf.keras.Sequential([
        tf.keras.layers.Dense(units=2*units, activation='relu'),
        tf.keras.layers.Dense(units=units),
        tf.keras.layers.Dropout(rate=dropout_rate),
    ])

    self.layernorm = tf.keras.layers.LayerNormalization()
#________________________________________________________________________________
  def call(self, seqs):
    seqs = seqs + self.seq(seqs)
    return self.layernorm(seqs)

#_______________________________________________________________________________________

class DecoderLayer(tf.keras.layers.Layer):
  def __init__(self, units, num_heads=1, dropout_rate=0.1):
    super().__init__()

    self.self_attention = CausalSelfAttention(num_heads=num_heads,
                                              key_dim=units,
                                              dropout=dropout_rate)
    self.cross_attention = CrossAttention(num_heads=num_heads,
                                          key_dim=units,
                                          dropout=dropout_rate)
    self.ff = FeedForward(units=units, dropout_rate=dropout_rate)

#_________________________________________________________________________________________

  def call(self, inputs, training=False):
    features, seqs = inputs

    # Text input
    seqs = self.self_attention(seqs)

    out_seq = self.cross_attention(seqs, features)
    self.last_attention_scores = self.cross_attention.last_attention_scores
    out_seq = self.ff(out_seq)
    return out_seq

#______________________________________________________________________________________________

class TokenOutput(tf.keras.layers.Layer):
  def __init__(self, tokenizer,vocab_size , banned_tokens=(' ', '<unk>', '<start>'), **kwargs):
    super().__init__()


    self.vocab_size = vocab_size
    self.dense = tf.keras.layers.Dense(
        units=vocab_size, **kwargs)
    self.tokenizer = tokenizer
    self.banned_tokens = banned_tokens

    self.bias = None

#______________________________________________________________________________________________________________

  def adapt(self):
    counts_arr = np.zeros(shape=(self.vocab_size,))

    for word, ct in self.tokenizer.word_counts.items():
      idx = self.tokenizer.word_index[word]
      counts_arr[idx] = ct

    # setting freq of bad_tokens = 0
    for word in self.banned_tokens:
      idx = self.tokenizer.word_index[word]
      counts_arr[idx] = 0

    # calculating prob. distr. over words in vocab
    total = counts_arr.sum()
    p = counts_arr/total

    # calculating entropy of vocab_distr
    p[counts_arr==0] = 1.0
    log_p = np.log(p)  # log(1) == 0
    entropy = -(log_p*p).sum()

    print()
    print(f"Uniform entropy: {np.log(self.vocab_size):0.2f}")
    print(f"Marginal entropy: {entropy:0.2f}")

    self.bias = log_p
    self.bias[counts_arr==0] = -1e9         # setting very -ve bias for banned tokens

#___________________________________________________________________________________________________________________

  def call(self, x):
    x = self.dense(x)
    # An Add layer doesn't work because of the different shapes.
    # This clears the mask, that's okay because it prevents keras from rescaling
    # the losses.
    return x + self.bias

#_______________________________________________________________________________________________________________

class Captioner(tf.keras.Model):

  def __init__(self,max_length, vocab_size, tokenizer, num_layers=1,
               units=256, embedding_dim =256,  num_heads=1, dropout_rate=0.1):
    super().__init__()

    self.max_length = max_length
    self.resnet_encoder = Resnet_Encoder(embedding_dim = embedding_dim)

    self.seq_embedding = SeqEmbedding(
        vocab_size=vocab_size,
        embedding_dim = embedding_dim,
        depth=units,
        max_length=max_length)

    self.decoder_layers = [
        DecoderLayer(units, num_heads=num_heads, dropout_rate=dropout_rate)
        for n in range(num_layers)]


    output_layer = TokenOutput(tokenizer, vocab_size, banned_tokens=(' ' , '<unk>', '<start>'))
    # This might run a little faster if the dataset didn't also have to load the image data.
    output_layer.adapt()
    self.output_layer = output_layer

    self.tokenizer = tokenizer

#____________________________________________________________________________
  def call(self, inputs):
    features, txt = inputs
    features = self.resnet_encoder(features)
    txt = self.seq_embedding(txt)
    # Look at the image
    for dec_layer in self.decoder_layers:
      txt = dec_layer(inputs=(features, txt))

    txt = self.output_layer(txt)
    return txt

#____________________________________________________________________________________________

  def simple_gen(self, features, temperature=1):
    initial = self.tokenizer.word_index([['[START]']]) # (batch, sequence)
    tokens = initial # (batch, sequence)
    for n in range(self.max_length):
      preds = self.call((features, tokens)).numpy()          # (batch, sequence, vocab)
      preds = preds[:,-1, :]                                 #(batch, vocab), picking last word

      #------------------------------------------------------------
      if temperature==0:
          next = tf.argmax(preds, axis=-1)[:, tf.newaxis]  # (batch, 1)
      else:
          next = tf.random.categorical(preds/temperature, num_samples=1)  # (batch, 1)
      #-------------------------------------------------------------

      tokens = tf.concat([tokens, next], axis=1) # (batch, sequence)

      if next[0] == self.tokenizer.word_to_index('[END]'):
        break

    words = self.tokenizer.index_to_word(tokens[0, 1:-1])
    result = tf.strings.reduce_join(words, axis=-1, separator=' ')

    return result.numpy().decode()
